{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTALLING PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we start by importing the necessary libraries for data manipulation and viz\n",
    "## !pip install seaborn\n",
    "## !pip install statsmodels\n",
    "\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline \n",
    "# Esse comando serve para plotar os gráficos estáticos logo abaixo da célula,\n",
    "\n",
    "# existem outras configurações do %matplolib que podem mostrar os gráficos em outras abas ou gráficos dinâmicos.\n",
    "# Por padrão, desde a versão 3.7 do python anaconda, a configuração padrão do %matplotlib já é o inline.\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/plotting.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding some ML capabilities with Scikit-learn\n",
    "## !pip install scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLUMNS DESCRIPTION**\n",
    "\n",
    ". passengerid = ID do passageiro do navio (código primário).\n",
    "\n",
    ". survived = Se sobreviveu ao naufrágio estará como 1 e caso esteja com 0 (zero) não sobreviveu.\n",
    "\n",
    ". pclass = Tipo de classe de passagem (Do 1 ao 3), (1 = 1st, 2 = 2nd, 3 = 3rd).\n",
    "\n",
    ". name = Nome do passageiro\n",
    "\n",
    ". sex = Gênero do passageiro, sendo masculino e feminino.\n",
    "\n",
    ". age = Idade do passageiro na data da ocorrência do naufrágio.\n",
    "\n",
    ". sibsp = Número de irmãos / cônjuges a bordo.\n",
    "\n",
    ". parch = Número de pais / filhos a bordo.\n",
    "\n",
    ". ticket = Código do ticket.\n",
    "\n",
    ". fare = Valor da passagem.\n",
    "\n",
    ". cabin = Código de identificação da Cabine.\n",
    "\n",
    ". embarked = Local ondem o passageiro embarcou no navio: C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train dataset and verifying the first info\n",
    "titanic = pd.read_csv('train.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing for the test dataset\n",
    "titanic_test = pd.read_csv('test.csv')\n",
    "titanic_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLORATORY ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some detail on our columns\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the NAs\n",
    "titanic.isna().sum()\n",
    "\n",
    "#isna() returns true (or 1) when the value is non existent (NaN) then we can .sum() the colum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or, to be more complete, let's see the proportion of NaN in each column\n",
    "pd.DataFrame(\n",
    "    zip(    ##zip joins two tupples\n",
    "        titanic.isna().sum(),               ##first column\n",
    "        titanic.isna().sum()/len(titanic)   ##second column\n",
    "    ),\n",
    "    columns = ['Count', 'Proportion'],\n",
    "    index = titanic.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see some quantitative description of our dataset\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our target variable is the \"Survived\" column. Let's see how many people survived\n",
    "\n",
    "titanic.Survived.value_counts()/len(titanic)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INITIALIZING PRE-PROCESSING**\n",
    "\n",
    "Starting by KDD process - Knowledge Discovery in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('kdd.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I always like to start with pairplot. It shows the distribution of some variables and we can vizualise the possible correlation between them\n",
    "# but in this case is not that impressive\n",
    "sb.pairplot(titanic[['Survived','Age','Fare','Sex', 'Pclass']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seaborn version:\", sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets check the variable fare\n",
    "\n",
    "##sb.histplot(data = titanic, x=\"Fare\")\n",
    "sb.histplot(titanic['Fare'])\n",
    "\n",
    "\n",
    "## seaborn is a great library for image plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(data = titanic, x = \"Survived\", y = \"Fare\")\n",
    "plt.title(\"Fare distribution for survivals and non survivals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's eliminate some outliers from the \"Fare\" columns\n",
    "\n",
    "titanic.loc[titanic['Fare']>=300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can truncate these three values to the maximum of Fare = 300\n",
    "\n",
    "titanic.loc[titanic['Fare']>=300, 'Fare'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating the same plot as before\n",
    "sb.boxplot(data = titanic, x = \"Survived\", y = \"Fare\")\n",
    "plt.title(\"Fare distribution for survivals and non survivals - Truncating outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the age of passengers\n",
    "\n",
    "sb.histplot(data = titanic, x = 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(data = titanic, y = 'Age', x = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUSBSTITUTING NaN VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completing NaN values\n",
    "print('Age info:\\nAverage= {} \\nMedian = {}'.format(titanic['Age'].mean(), titanic['Age'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluating for sex\n",
    "C_median = titanic['Age'].groupby(by= titanic.Sex).median()\n",
    "\n",
    "C_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluating for class\n",
    "C_median = titanic['Age'].groupby(by= titanic.Pclass).median()\n",
    "\n",
    "C_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will find the mean age for each class/sex group\n",
    "\n",
    "trainMeans = titanic.groupby(['Sex','Pclass'])['Age'].mean()\n",
    "\n",
    "trainMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the averages\n",
    "\n",
    "def age_estimate(x):\n",
    "    if not np.isnan(x['Age']):                  ## if age is not NaN\n",
    "        return x['Age']                         ## return itself (the age)\n",
    "    return trainMeans[x[\"Sex\"], x['Pclass']]    ## otherwise retuns the age calculated in the trainMeans formula\n",
    "\n",
    "\n",
    "titanic['Age'] = titanic.apply(age_estimate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUALITATIVE PREDICTIVE VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluatint effect of \"Sex\"\n",
    "titanic.groupby('Survived')['Sex'].value_counts().unstack(0).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluatint effect of where the passager embarked\n",
    "titanic.groupby('Survived')['Embarked'].value_counts().unstack(0).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NAN (only 2 values) of the \"Embarked\" column with the Mode\n",
    "\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING DUMMIES FOR QUALITATIVE VARIABLES**\n",
    "\n",
    "Some algorithms have dificulties in evaluating categorical values. \n",
    "Specially if the category is represented as a numerical value.\n",
    "In this case, one strategy is to create what we call dummy columns, one hot enconding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('hot encoding dummy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'get_dummies' method will evaluate the column selected and return True or false for each possible value\n",
    "pd.get_dummies(titanic['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 'dropfirst' is commonly used, because you if you have n possible results...\n",
    "#you can determine all the values with n-1 columns (i.e, if all columns return False, the dropped column would be True)\n",
    "pd.get_dummies(titanic['Sex'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column that says if the passanger is male or not\n",
    "titanic['male'] = pd.get_dummies(\n",
    "    titanic['Sex'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the same for the embarked place\n",
    "embark_dummies = pd.get_dummies(\n",
    "    titanic['Embarked'], #This time we will do the same for the embarked column\n",
    "    drop_first=True, #dropping the first column (should be C)\n",
    "    prefix='Embarked_' #putting a prefix so we end up with Embarked_Q and Embarked_S columns\n",
    ")\n",
    "\n",
    "embark_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding these columns to my dataframe with the concat method\n",
    "titanic = pd.concat(\n",
    "    [ titanic , embark_dummies ],   #the two dataframes we want to unite\n",
    "    axis=1                          #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we do not need the \"Sex\" or \"Embarked\" columns anymore\n",
    "titanic.drop(\n",
    "    ['Sex','Embarked'],     # the columns to be dropped\n",
    "    axis = 1,               # the axis of drop (1 means column)\n",
    "    inplace=True            # means we will substitute the original dataframe\n",
    ")\n",
    "\n",
    "#The inplace true means we will replace the original dataframe. It is the same as if we had typed:\n",
    "#titanic = titanic.drop(['Sex','Embarked'],axis = 1)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORKING ON OTHER QUALITATIVE VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's review where we are so far\n",
    "\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass is described as Int64 because the value is numerical (1, 2 or 3)\n",
    "#But we do not want our model to view it as a quantitative value\n",
    "#so we change to class\n",
    "\n",
    "titanic['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Pclass'] = titanic['Pclass'].astype('category')\n",
    "titanic['Survived'] = titanic['Survived'].astype('bool')\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's create our dummies for Pclass\n",
    "pclass_dummies = pd.get_dummies(\n",
    "    titanic['Pclass'],              #Create a dummy that returns the columns \n",
    "    drop_first=True,                #Droppint the Pclass_1 column that will not be necessary\n",
    "    prefix=\"Pclass_\"                #Pclass_2 and #Pclass_3\n",
    ")\n",
    "\n",
    "titanic = pd.concat(\n",
    "    [ titanic , pclass_dummies ],   #the two dataframes we want to unite\n",
    "    axis=1                          #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "titanic.drop(\n",
    "    ['Pclass'],                     #with the dummies ready, we do not need the original Pclass column\n",
    "    axis = 1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##finally, let's drop the individual values that will not contribute to the machine learning\n",
    "titanic_train = titanic.drop(['PassengerId','Name','Cabin', 'Ticket'], axis=1)\n",
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEPARATING TEST AND TRAIN DATA**\n",
    "\n",
    "In this phase we divide our train model in two parts\n",
    "\n",
    "one part will be used to train the model\n",
    "\n",
    "the other part will evaluate the assertiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using the train_test_split from SKLEARN\n",
    "\n",
    "'''\n",
    "(function) def train_test_split(\n",
    "    *arrays: Any,\n",
    "    test_size: Float | None = None,\n",
    "    train_size: Float | None = None,\n",
    "    random_state: Int | RandomState | None = None,\n",
    "    shuffle: bool = True,\n",
    "    stratify: ArrayLike | None = None\n",
    ") -> list\n",
    "\n",
    "Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "Quick utility that wraps input validation, next(ShuffleSplit().split(X, y)), \n",
    "and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.\n",
    "'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    titanic_train.drop('Survived',axis=1),          # independent values, will be the dataframe without the target column\n",
    "    titanic_train['Survived'],                      # dependent value, target column\n",
    "    test_size=0.10,                                 # how much of the dataframe will be used for testing (in this case 90% will be used for training)\n",
    "    random_state=10                                 # including a random state just ensures the result of the shuffle will always be the same for this block\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##X_train is the matrix of values that the model will use to try and understand the behaviour of y_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##after the training, the model will try to apply the resulting formula into the X_test values, and see if got correct results when comparing\n",
    "##to the y_test results\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST MODEL - DECISION TREE**\n",
    "\n",
    "HYPERPARAMETERS\n",
    "\n",
    "tree = DecisionTreeClassifier(\n",
    "\n",
    "criterion='gini', # gini vem por default, mas podemos optar por entropy\n",
    "\n",
    "splitter='best', # a estratégia utilizada para fazer a separação de cada nó # ela também pode ser feita de forma randômica utilizando 'random'\n",
    "\n",
    "max_depth= None, # a máxima profundida que sua árvore de decisão pode ter # se for None ela vai buscar a máxima pureza possível\n",
    "\n",
    "min_samples_split = 2, # o mínimo de registros necessários para que uma separação seja feita\n",
    "\n",
    "min_samples_leaf = 1, # o mínimo de registros necessários em cada nós-folha (veja a primeira imagem)\n",
    "\n",
    "max_features = None, # o número de atributos que será considerado durante o split # None -> seleciona todos os atributos, 'sqrt' -> raiz quadrada do número dos atributos, 'log2' -> log de base 2 do número de atributos\n",
    "\n",
    "max_leaf_nodes=None, # a quantidade máxima de nós-folha que a árvore pode ter # se for None ele não limitará o número de nós-folha\n",
    "\n",
    "min_impurity_decrease=0.0, # o split irá ocorrer em cada nó se o decréscimo da impureza foi maior ou igual a este valor\n",
    "\n",
    "random_state= 17, # permite gerar a mesma amostra (o notebook ser reproduzível)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('Decision_Tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the DecisionTreeClassifier class\n",
    "\n",
    "'''\n",
    "class DecisionTreeClassifier(\n",
    "    *,\n",
    "    criterion: Literal['gini', 'entropy', 'log_loss'] = \"gini\",\n",
    "    splitter: Literal['best', 'random'] = \"best\",\n",
    "    max_depth: Int | None = None,\n",
    "    min_samples_split: float | int = 2,\n",
    "    min_samples_leaf: float | int = 1,\n",
    "    min_weight_fraction_leaf: Float = 0,\n",
    "    max_features: float | int | Literal['auto', 'sqrt', 'log2'] | None = None,\n",
    "    random_state: Int | RandomState | None = None,\n",
    "    max_leaf_nodes: Int | None = None,\n",
    "    min_impurity_decrease: Float = 0,\n",
    "    class_weight: Mapping | str | Sequence[Mapping] | None = None,\n",
    "    ccp_alpha: float = 0\n",
    ")\n",
    "'''\n",
    "\n",
    "Classif_tree = DecisionTreeClassifier(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the decision tree with our X_train matrix and y_train results\n",
    "\n",
    "classif = Classif_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the most important features\n",
    "\n",
    "classif.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Damn! Which is which here? Lets find out.\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train.columns)\n",
    "\n",
    "df2 = pd.DataFrame(classif.feature_importances_)\n",
    "\n",
    "df3 = pd.concat(\n",
    "    [df, df2],\n",
    "    axis =1\n",
    ")\n",
    "\n",
    "tree_importance = df3.set_axis(['Feature', 'Importance'], axis = 'columns').sort_values(by=['Importance'], ascending=False)\n",
    "\n",
    "tree_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting our y with the X_train and the classif model\n",
    "\n",
    "y_pred_all = classif.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_train, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Matriz de Confusão\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_train, y_pred_all)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_all,digits=2))\n",
    "\n",
    "# Resultado do classification_report:\n",
    "# Precision score = VP/(VP+FP)\n",
    "# Recall score = VP/(VP+FN)\n",
    "# F1 Score = 2* Precision Score * Recall Score/ (Precision Score + Recall Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATING THE RESULTS**\n",
    "\n",
    "Seems good right? Very high precision, recall and F1 score....\n",
    "\n",
    "Well, actually not. In the decision tree if we \"let it go\" as further as it wants, it will basically explain every individual case by its own parameters\n",
    "\n",
    "Usually if we get a precision higher than 0.8, we should suspect something is too good to be true\n",
    "\n",
    "Let's plot the current tree so we can see what we are talking about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o fig e o axes  - selecionando alguns niveis\n",
    "fig, ax = plt.subplots(figsize=(16,12),dpi=130)\n",
    "#criando o plot\n",
    "plot_tree(classif, # a decision tree que será plotada\n",
    "          feature_names =(['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'male', 'Embarked_C','Embarked_Q', 'Embarked_S',\n",
    "                           'Pclass_1', 'Pclass_2', 'Pclass_3']), # trará o nome das features utilizadas na primeira linha de cada nó\n",
    "          ax=ax, # plotamos no axes criado do matplotlib\n",
    "          precision=1, # precisão dos valores numéricos\n",
    "          filled=True,\n",
    "          max_depth=4, #  escolhemos a profundidade da árvore\n",
    "          proportion = True, # retorna a proporção dos valores das amostras\n",
    "          fontsize = 12 # mudar o tamanho da fonte\n",
    "        )\n",
    "#plotando o gráfico\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And down it goes... we should get the most important features and limit how much we want our decision tree to go down...\n",
    "\n",
    "#Let's go back and do the DecisionTreeClassifier again... but now using the available parameters\n",
    "\n",
    "'''\n",
    "class DecisionTreeClassifier(\n",
    "    *,\n",
    "    criterion: Literal['gini', 'entropy', 'log_loss'] = \"gini\",\n",
    "    splitter: Literal['best', 'random'] = \"best\",\n",
    "    max_depth: Int | None = None,\n",
    "    min_samples_split: float | int = 2,\n",
    "    min_samples_leaf: float | int = 1,\n",
    "    min_weight_fraction_leaf: Float = 0,\n",
    "    max_features: float | int | Literal['auto', 'sqrt', 'log2'] | None = None,\n",
    "    random_state: Int | RandomState | None = None,\n",
    "    max_leaf_nodes: Int | None = None,\n",
    "    min_impurity_decrease: Float = 0,\n",
    "    class_weight: Mapping | str | Sequence[Mapping] | None = None,\n",
    "    ccp_alpha: float = 0\n",
    ")\n",
    "'''\n",
    "\n",
    "Classif_tree_optimized = DecisionTreeClassifier(\n",
    "    random_state=10,\n",
    "    max_depth = 4\n",
    ")\n",
    "\n",
    "classif2 = Classif_tree_optimized.fit(X_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(X_train.columns)\n",
    "\n",
    "df2 = pd.DataFrame(classif2.feature_importances_)\n",
    "\n",
    "df3 = pd.concat(\n",
    "    [df, df2],\n",
    "    axis =1\n",
    ")\n",
    "\n",
    "tree_importance = df3.set_axis(['Feature', 'Importance'], axis = 'columns').sort_values(by=['Importance'], ascending=False)\n",
    "\n",
    "tree_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING THE DECISION TREE IN OUR TEST.CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's train our new model\n",
    "y_pred2 = classif2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_train, y_pred2)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred2,digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test this model in our test.csv file\n",
    "\n",
    "test_decision_tree = pd.read_csv('test.csv')\n",
    "\n",
    "test_decision_tree.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling up the NaN Ages\n",
    "trainMeans2 = test_decision_tree.groupby(['Sex','Pclass'])['Age'].mean()\n",
    "\n",
    "def age_estimate(x):\n",
    "    if not np.isnan(x['Age']):                  ## if age is not NaN\n",
    "        return x['Age']                         ## return itself (the age)\n",
    "    return trainMeans2[x[\"Sex\"], x['Pclass']]    ## otherwise retuns the age calculated in the trainMeans formula\n",
    "\n",
    "\n",
    "test_decision_tree['Age'] = test_decision_tree.apply(age_estimate, axis=1)\n",
    "test_decision_tree.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the NaN Fare\n",
    "trainfareAverage = test_decision_tree.groupby(['Sex','Pclass'])['Fare'].mean()\n",
    "\n",
    "def fare_estimate(x):\n",
    "    if not np.isnan(x['Fare']):                  ## if age is not NaN\n",
    "        return x['Fare']                         ## return itself (the age)\n",
    "    return trainfareAverage[x[\"Sex\"], x['Pclass']]    ## otherwise retuns the age calculated in the trainMeans formula\n",
    "\n",
    "\n",
    "test_decision_tree['Fare'] = test_decision_tree.apply(fare_estimate, axis=1)\n",
    "test_decision_tree.info()\n",
    "\n",
    "#fare = Average\n",
    "#test_decision_tree['Fare'].fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting our dummies\n",
    "\n",
    "## Sex Dummies\n",
    "test_decision_tree['male'] = pd.get_dummies(\n",
    "    test_decision_tree['Sex'],\n",
    "    drop_first=True\n",
    ")\n",
    "## EMBARK DUMMIES\n",
    "embark_dummies2 = pd.get_dummies(\n",
    "    test_decision_tree['Embarked'],                 #This time we will do the same for the embarked column\n",
    "    drop_first=True,                                #dropping the first column (should be C)\n",
    "    prefix='Embarked_'                              #putting a prefix so we end up with Embarked_Q and Embarked_S columns\n",
    ")\n",
    "\n",
    "test_decision_tree = pd.concat(\n",
    "    [ test_decision_tree , embark_dummies2 ],    #the two dataframes we want to unite\n",
    "    axis=1                                       #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "##P_Class Dummies\n",
    "pclass_dummies2 = pd.get_dummies(\n",
    "    test_decision_tree['Pclass'],              #Create a dummy that returns the columns \n",
    "    drop_first=True,                #Droppint the Pclass_1 column that will not be necessary\n",
    "    prefix=\"Pclass_\"                #Pclass_2 and #Pclass_3\n",
    ")\n",
    "\n",
    "test_decision_tree = pd.concat(\n",
    "    [ test_decision_tree , pclass_dummies2 ],   #the two dataframes we want to unite\n",
    "    axis=1                          #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "test_decision_tree.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_decision_tree[['Age','SibSp','Parch','Fare','male','Embarked__Q','Embarked__S', 'Pclass__2','Pclass__3']]\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decision_tree['Survived'] = classif2.predict(test_decision_tree[['Age','SibSp','Parch','Fare','male','Embarked__Q','Embarked__S', 'Pclass__2','Pclass__3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decision_tree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decision_tree['Survived'] = test_decision_tree['Survived'].astype(int)\n",
    "\n",
    "kaggle_response = test_decision_tree[['PassengerId','Survived']].sort_values(by = 'PassengerId')\n",
    "\n",
    "kaggle_response.head(418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_response.to_csv('decision_tree_response.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND MODEL - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2 = titanic\n",
    "titanic2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try to use the Logistic regression with the 3 most important variables, but first we turn the target to numeric\n",
    "titanic2['Survived']= titanic2['Survived'].astype(int)\n",
    "titanic2['Pclass__3']= titanic2['Pclass__3'].astype(int)\n",
    "titanic2['Pclass__2']= titanic2['Pclass__2'].astype(int)\n",
    "titanic2['male']= titanic2['male'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smf is the stat model \n",
    "modelo = smf.glm(formula='Survived ~ male + Pclass__3 + Age + Pclass__2', data=titanic2,\n",
    "               family = sm.families.Binomial()).fit()\n",
    "print(modelo.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good results for the p-values. Which are all bellow 0.05 for 95% confidence\n",
    "#this results shows that,from the baselines of our dummies, that all the selected variables  have negative coeficients\n",
    "#remember that in our case, the target variable is 1 (survived) or 0 (did not survive)\n",
    "# that means that being male, being on any class other than first, and being older decreased the chance of surviving\n",
    "# the fact that both Pclass__3 and Pclass__2 have positive coefficients, shows that it being\n",
    "\n",
    "print(np.exp(modelo.params[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.exp(modelo.params[1:]) - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that's easier to understand\n",
    "# we interpret that as:\n",
    "# men have 92% less chance to survive than women\n",
    "# people from third class had 92% less change\n",
    "# each year a passenger were, less 3.7% chance of surviving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(penalty=None, solver = 'newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's separate only the main factors\n",
    "baseline_df = titanic2[['Survived','Pclass__3', 'Pclass__2', 'Age', 'male']]\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = baseline_df['Survived']\n",
    "X = baseline_df[['Pclass__3', 'Pclass__2', 'Age', 'male']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same values found before\n",
    "accuracy = accuracy_score(y, log_reg_model.predict(X))\n",
    "print('The model got %0.4f of accuracy.' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, log_reg_model.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizendo as probabilidades\n",
    "yhat = log_reg_model.predict_proba(X) \n",
    "print('AUC: %0.2f' % roc_auc_score(y, yhat[:, 1]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, figsize=(10,6)):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    plt.figure(figsize=figsize)\n",
    "    auc_value = roc_auc_score(y_true, y_score)\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC curve (area = %0.2f)' % auc_value)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y, yhat[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test.csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test this model in our test.csv file------------------------------------------------\n",
    "\n",
    "test_decision_tree = pd.read_csv('test.csv')\n",
    "\n",
    "#Filling up the NaN Ages------------------------------------------------------------------\n",
    "trainMeans2 = test_decision_tree.groupby(['Sex','Pclass'])['Age'].mean()\n",
    "\n",
    "def age_estimate(x):\n",
    "    if not np.isnan(x['Age']):                  ## if age is not NaN\n",
    "        return x['Age']                         ## return itself (the age)\n",
    "    return trainMeans2[x[\"Sex\"], x['Pclass']]    ## otherwise retuns the age calculated in the trainMeans formula\n",
    "\n",
    "\n",
    "test_decision_tree['Age'] = test_decision_tree.apply(age_estimate, axis=1)\n",
    "\n",
    "\n",
    "#filling the NaN Fare-------------------------------------------------------------------------\n",
    "trainfareAverage = test_decision_tree.groupby(['Sex','Pclass'])['Fare'].mean()\n",
    "\n",
    "def fare_estimate(x):\n",
    "    if not np.isnan(x['Fare']):                  ## if age is not NaN\n",
    "        return x['Fare']                         ## return itself (the age)\n",
    "    return trainfareAverage[x[\"Sex\"], x['Pclass']]    ## otherwise retuns the age calculated in the trainMeans formula\n",
    "\n",
    "\n",
    "test_decision_tree['Fare'] = test_decision_tree.apply(fare_estimate, axis=1)\n",
    "\n",
    "#fare = Average\n",
    "#test_decision_tree['Fare'].fillna()\n",
    "\n",
    "#Getting our dummies-----------------------------------------------------------------------------\n",
    "\n",
    "## Sex Dummies\n",
    "test_decision_tree['male'] = pd.get_dummies(\n",
    "    test_decision_tree['Sex'],\n",
    "    drop_first=True\n",
    ")\n",
    "## EMBARK DUMMIES\n",
    "embark_dummies2 = pd.get_dummies(\n",
    "    test_decision_tree['Embarked'],                 #This time we will do the same for the embarked column\n",
    "    drop_first=True,                                #dropping the first column (should be C)\n",
    "    prefix='Embarked_'                              #putting a prefix so we end up with Embarked_Q and Embarked_S columns\n",
    ")\n",
    "\n",
    "test_decision_tree = pd.concat(\n",
    "    [ test_decision_tree , embark_dummies2 ],    #the two dataframes we want to unite\n",
    "    axis=1                                       #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "##P_Class Dummies\n",
    "pclass_dummies2 = pd.get_dummies(\n",
    "    test_decision_tree['Pclass'],              #Create a dummy that returns the columns \n",
    "    drop_first=True,                #Droppint the Pclass_1 column that will not be necessary\n",
    "    prefix=\"Pclass_\"                #Pclass_2 and #Pclass_3\n",
    ")\n",
    "\n",
    "test_decision_tree = pd.concat(\n",
    "    [ test_decision_tree , pclass_dummies2 ],   #the two dataframes we want to unite\n",
    "    axis=1                          #the axis=1 indicate we will concatenate the dataframes horizontally (add columns)\n",
    ")\n",
    "\n",
    "test_decision_tree.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg = test_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember what columsn our baseline used:\n",
    "\n",
    "## X = baseline_df[['Pclass__3', 'Pclass__2', 'Age', 'male']]\n",
    "\n",
    "test_log_reg_X = test_log_reg[['Pclass__3', 'Pclass__2', 'Age', 'male']] ## let's use this df that is already transformed for us\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model.predict(test_log_reg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg['Survived'] = log_reg_model.predict(test_log_reg_X).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_response2 = test_log_reg[['PassengerId','Survived']].sort_values(by = 'PassengerId')\n",
    "kaggle_response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_response2.to_csv('Logistic_regression_response.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
